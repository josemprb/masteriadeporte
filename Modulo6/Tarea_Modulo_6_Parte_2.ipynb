{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea_Modulo_6_Parte_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itY3q17Uba-N"
      },
      "source": [
        "# Tarea Individual Módulo 6 - Parte 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDvouw8R7ch2"
      },
      "source": [
        "El entorno virtual de Google Colab tiene instaladas la mayoría de las librerías\n",
        "que se suelen utilizar en problemas de IA. Sin embargo, necesitamos instalar la\n",
        "librería transformers de Huggingface y la librería tqdm que es una librería que nos permite mostrar una barra de progreso cuando utlizamos bucles for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4fGFUifZ54"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCzcGoUEf1ks"
      },
      "source": [
        "# 0. Imports\n",
        "\n",
        "En la sección `Imports` agrupamos todas las librerías y clases que debemos importar en esta tarea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF09g3M6fsNO"
      },
      "source": [
        "# import basic libraries for data science\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import torch and transformers\n",
        "import torch\n",
        "from torch import cuda\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# import different metrics for evaluation\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# import tqdm to track progress\n",
        "from tqdm import tqdm\n",
        "\n",
        "# import some python modules required for some of the utility functions\n",
        "import itertools\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nET1o0yR9d1E"
      },
      "source": [
        "# 1. Utils\n",
        "\n",
        "En la sección `Utils` se encuentran varias funciones de utilidad con sus docstrings que se utilizarán a largo de la implementación para preprocesar y limpiar datos y también evaluar los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO-4gPPE9fUV"
      },
      "source": [
        "################################\n",
        "# text processing and cleaning #\n",
        "################################\n",
        "\n",
        "def encode_sentiment(sentiment):\n",
        "  \"\"\"\n",
        "  Label encode sentiment.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentiment: str, sentiment {\"positive\", \"sentiment\"}\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  int, 1's for positive and 0's for negative\n",
        "\n",
        "  \"\"\"\n",
        "  if sentiment == \"Positive\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def is_ascii(w):\n",
        "  \"\"\"\n",
        "  Check if character is ascii type.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  w: str, character\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  bool, True if character is ascii. False otherwise.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    w.encode(\"ascii\")\n",
        "    return True\n",
        "  except UnicodeEncodeError:\n",
        "    return False\n",
        "\n",
        "def text_cleaning(text):\n",
        "  \"\"\"\n",
        "  Clean text from symbols, punctuation, etc.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text: string, text data\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  cleaned_text: string, cleaned text data\n",
        "  \"\"\"\n",
        "  # remove string formatting '\\n' or '\\t'\n",
        "  tmp_text = re.sub(r'\\n+', '. ', text)\n",
        "  tmp_text = re.sub(r'\\t+', '. ', text)\n",
        "  # remove words with non-ascii characters\n",
        "  tmp_text = \" \".join([word for word in tmp_text.split() if is_ascii(word)])\n",
        "  # remove email address\n",
        "  tmp_text = \" \".join([word for word in tmp_text.split() if not word.startswith(\"@\")])\n",
        "  # remove urls\n",
        "  tmp_text = re.sub(r'http\\S+', '', tmp_text, flags=re.MULTILINE)\n",
        "  tmp_text = re.sub(r'www\\S+', '', tmp_text, flags=re.MULTILINE)\n",
        "  # remove punctuation but . (to split sentences)\n",
        "  cleaned_text = re.sub('[^A-Za-z.,]+', ' ', tmp_text)\n",
        "  # lowercase\n",
        "  cleaned_text = cleaned_text.lower()\n",
        "\n",
        "  return cleaned_text\n",
        "\n",
        "##############\n",
        "# Evaluation #\n",
        "##############\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label',fontsize=15)\n",
        "    plt.xlabel('Predicted label',fontsize=15)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VosJQ0fegIjW"
      },
      "source": [
        "# 2. Config\n",
        "\n",
        "En la sección `Config` definimos variables variables y objetos que utilizaremos en el desarrollo y queremos tener agrupados en el mismo lugar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7kdLbAqQIi0"
      },
      "source": [
        "# Defining some key variables that will be used later on\n",
        "MAX_LEN = 256 # max number of tokens\n",
        "BATCH_SIZE = 64\n",
        "SEED = 42 # for reprodudible results\n",
        "\n",
        "# Initialiaze model tokenizer\n",
        "TOKENIZER = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "# Setting up the device for GPU usage if available\n",
        "DEVICE = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNb4a1Gn9vGj"
      },
      "source": [
        "# 3. Data\n",
        "\n",
        "En la sección `Data` cargamos el dataset en un dataframe de pandas y codificamos la columna sentimiento.\n",
        "\n",
        "Además, procedemos a la limpieza del texto utlizando la función `text_cleaning()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfEF5fxD9ubi"
      },
      "source": [
        "# load dataset\n",
        "raw_data = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/MASTERS/DEPORTE/DATA/football_tweets_train.csv\", \\\n",
        "    encoding='utf-8'\n",
        "    ).drop(\"id\", axis=1)\n",
        "raw_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4KIcmcGgNdB"
      },
      "source": [
        "print(f\"Number of tweets: {raw_data.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR6wTD0RlbBk"
      },
      "source": [
        "raw_data.polarity.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id8Tz_aIlOHZ"
      },
      "source": [
        "Queremos comparar el desempeño del modelo utilizado en la parte 1 de la tarea. Este modelo estaba ajustado para una tarea de clasificación binaria. Sin embargo, vemos como este dataset esta preparado para un problema de clasificación multiclase.\n",
        "\n",
        "Nos quedaremos únicamente con los ejemplos positivos y negativos para poder realizar la comparación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFFM8Y3elxKo"
      },
      "source": [
        "# get rid of neutral examples\n",
        "data_subset = raw_data.loc[raw_data.polarity != \"Neutral\"]\n",
        "data_subset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCCBiAOShQF_"
      },
      "source": [
        "# encode sentiment\n",
        "data_subset['polarity'] = data_subset['polarity'].map(encode_sentiment)\n",
        "data_subset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf70PkZF_ikL"
      },
      "source": [
        "### clean tweets\n",
        "cleaned_data = data_subset.copy().reset_index(drop=True)\n",
        "cleaned_data[\"text\"] = cleaned_data[\"text\"].map(text_cleaning)\n",
        "cleaned_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYFL3kpfhGfY"
      },
      "source": [
        "# 4. Dataset and Dataloader\n",
        "\n",
        "En la sección `Dataset and Dataloader` vamos a crear un dataset de Pytorch del tipo map-iterable que utilizaremos para almacenar los datos y procesarlos de acuerdo a los requirimientos del modelo.\n",
        "\n",
        "Después creamos un objeto `DataLoader` de Pytorch que toma el dataset para poder pasar ejemplos al modelo en lotes o batches.\n",
        "\n",
        "__Clase DistilBertDataset__\n",
        "\n",
        "La clase `DistilBertDataset` toma como argumentos un dataframe con los datos, el tokenizador del modelo y el número de máximo de tokens que definimos en la sección `Config`.\n",
        "\n",
        "El método `__getitem__()` toma como parámetro un índice de los datos en el dataset, extrae el ejemplo correspondiente del dataframe y utiliza la función `encode_plus()` del tokenizador para procesar los inputs de acuerdo a los requerimientos de DistilBERT ( special tokens [CLS] y [SEP], truncation, padding, etc.). Este método devulce un diccionario de python con los tensores que continen los inputs ids, los attention masks y los el sentimiento codificado.\n",
        "\n",
        "El método `__len__()` simplemente devuelve el número de ejemplos en el dataset.\n",
        "\n",
        "__DataLoader__\n",
        "\n",
        "El `DataLoader` viene a ser un generador que divide el dataset en lotes de tamaño `BATCH_SIZE` para alimentar el modelo.\n",
        "\n",
        "En los parámetros podemos definir el tamaño de lote o batch size, si seleccionar de forma aleatoria los ejemplos a incluir en el batch o no (recomendable al realizar un entrenamiento) y también el número de workers (2 para Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgA9BXnjhlAj"
      },
      "source": [
        "class DistilBertDataset(Dataset):\n",
        "  \"\"\"Custom pytorch map-iterable dataset for sentiment analysis with DistilBERT.\"\"\"\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "      self.len = len(dataframe)\n",
        "      self.data = dataframe\n",
        "      self.tokenizer = tokenizer\n",
        "      self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      text = str(self.data['text'].loc[index])\n",
        "      text = \" \".join(text.split())\n",
        "      inputs = self.tokenizer.encode_plus(\n",
        "          text,\n",
        "          None,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          padding='max_length',\n",
        "          return_token_type_ids=False,\n",
        "          truncation=True\n",
        "      )\n",
        "      ids = inputs['input_ids']\n",
        "      mask = inputs['attention_mask']\n",
        "      \n",
        "      return {\n",
        "          'ids': torch.tensor(ids, dtype=torch.long),\n",
        "          'mask': torch.tensor(mask, dtype=torch.long),\n",
        "          'targets': torch.tensor(self.data['polarity'].loc[index], dtype=torch.long)\n",
        "      }\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.len"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maPFdrm-RStd"
      },
      "source": [
        "# create dataset\n",
        "dataset = DistilBertDataset(cleaned_data, TOKENIZER, MAX_LEN)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXjuKL1AGYCS"
      },
      "source": [
        "print(\"---- Visually inspecting 5th element ----\")\n",
        "print(f\"Input ids: {dataset[6]['ids']}\")\n",
        "print(f\"Attention masks: {dataset[6]['mask']}\")\n",
        "print(f\"Target: {dataset[6]['targets']}\")\n",
        "print(\"------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF9X4qSqiXzK"
      },
      "source": [
        "# define dataloader params\n",
        "dataloader_params = {'batch_size': BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 2\n",
        "                }\n",
        "\n",
        "# create dataloader\n",
        "data_loader = DataLoader(dataset, **dataloader_params)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYipI3FDigJZ"
      },
      "source": [
        "# 5. Model\n",
        "\n",
        "En la sección `Model` creamos la clase DistilBERTClass() con el modelo ya ajustado que se encuentra en el hub de modelos de Huggingface.\n",
        "\n",
        "El método `forward()` de la clase toma los inputs ids y attention mask que devuelve el método `__getitem__()` de la clase `DistilBertDataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY14bx9Cie9Z"
      },
      "source": [
        "class DistilBERTClass(torch.nn.Module):\n",
        "  \"\"\"Custom class for DilstilBERT model for Sequence Classification.\"\"\"\n",
        "  def __init__(self):\n",
        "      super(DistilBERTClass, self).__init__()\n",
        "      self.model = AutoModelForSequenceClassification \\\n",
        "        .from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "      output = self.model(input_ids=input_ids, attention_mask=attention_mask) \n",
        "      logits = output.logits\n",
        "      \n",
        "      return logits"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv-5Vd4b98iH"
      },
      "source": [
        "# 6. Inference\n",
        "\n",
        "En la sección `Inference` vamos a utilizar el modelo para realizar inferencia y obtener los sentimientos de los tweets.\n",
        "\n",
        "Utilizamos la función `inference()` para obtener los outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM-4udy2D8Jx"
      },
      "source": [
        "# Download and load trained DistilBERT model\n",
        "model = DistilBERTClass()\n",
        "model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWsImXd8aJVH"
      },
      "source": [
        "def inference(data_loader, model, device):\n",
        "    \"\"\"\n",
        "    Binary classification using DistilBERT model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_loader: Pytorch DataLoader object\n",
        "    model: DistilBERTClass Object\n",
        "    device: str, device\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    lists, outputs logits and targets\n",
        "    \"\"\"\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    # create lists to be populated with predictions and corresponding targets\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    # do not calculate gradients as not required for inference\n",
        "    with torch.no_grad():\n",
        "        # loop over batches and get predictions\n",
        "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            ids = d['ids']\n",
        "            mask = d['mask']\n",
        "            targets = d['targets']\n",
        "\n",
        "            # send them to the cuda device we are using\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets = targets.to(device, dtype=torch.long)\n",
        "            # get outputs logits\n",
        "            outputs = model(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask\n",
        "            )\n",
        "            # Normalize logits and store results and targets in lists\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "    return fin_outputs, fin_targets"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_h3497nHqNV"
      },
      "source": [
        "# run inference -> sentiment analysis\n",
        "outputs, targets = inference(\n",
        "    data_loader=data_loader,\n",
        "    model=model,\n",
        "    device=DEVICE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFdIcfRsHt4S"
      },
      "source": [
        "# convert normalized logits to sentiment -> Criteria: most probable class\n",
        "outputs = np.argmax(outputs, axis=1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mBJtMTWEdq8"
      },
      "source": [
        "# 7. Evaluation\n",
        "\n",
        "En la sección `Evaluation` calculamos métricas típicas de un poblema de clasificación binario: Accuracy y F1 Score.\n",
        "\n",
        "También graficamos la matriz de confusión utlizando la función de utilidad `plot_confusion_matrix()`.\n",
        "\n",
        "Finalmente, hacemos una pequeña evaluación visual de los inputs, los targets y las predicciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejQLUp8UHt2G"
      },
      "source": [
        "accuracy = accuracy_score(targets, outputs)\n",
        "f_score = f1_score(targets, outputs, average='macro')\n",
        "print(f\"Training Accuracy Score = {accuracy}\")\n",
        "print(f\"Training F1-Score = {f_score}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGYALx53QSPO"
      },
      "source": [
        "# classification report\n",
        "print(classification_report(targets, outputs, target_names=[\"Negative\", \"Positive\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfHfKlYFHtzv"
      },
      "source": [
        "# print the confusion matrix\n",
        "cnf_matrix = confusion_matrix(targets, outputs, labels=[0, 1])\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_confusion_matrix(\n",
        "    cnf_matrix,classes=['Negative','Positive'],\n",
        "    normalize=True,\n",
        "    title='Confusion matrix'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlwlH4rdQt5w"
      },
      "source": [
        "#visual evalution\n",
        "for tweet, cleaned_tweet, target, output in zip(data_subset.text.values[:10], cleaned_data.text.values[:10], targets, outputs):\n",
        "  print(f\"Original text: {tweet}\")\n",
        "  print(f\"Text: {cleaned_tweet}\")\n",
        "  print(f\"Target: {target}\\tOutput: {output}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWTinNQn_vC"
      },
      "source": [
        "# Celdas para explorar resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6VvdQNvHtaL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK_Xa4b9oAr2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKOSBREfoApV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36P9--lxoAmk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc5Qxg1woAjw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbCIx62VoAcO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
